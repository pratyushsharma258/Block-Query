{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7cf8237b-d50a-4dc9-a294-2bb422847ce0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cf8237b-d50a-4dc9-a294-2bb422847ce0",
        "outputId": "c3de19d4-6108-4e5b-9f71-66ef7aeb0532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Core ML Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input,\n",
        "    Embedding,\n",
        "    LSTM,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    LayerNormalization,\n",
        "    Bidirectional,\n",
        "    Layer,\n",
        "    Lambda,\n",
        "    Concatenate\n",
        ")\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping,\n",
        "    ReduceLROnPlateau,\n",
        "    ModelCheckpoint\n",
        ")\n",
        "\n",
        "# Data Processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Utility Libraries\n",
        "import time\n",
        "import datetime\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "190debc0-d689-432d-9bbf-44a497eaaedd",
      "metadata": {
        "id": "190debc0-d689-432d-9bbf-44a497eaaedd"
      },
      "source": [
        "# Bidirectional LSTM with attention"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b792c5b-bb43-432c-a168-8c435c85623a",
      "metadata": {
        "id": "6b792c5b-bb43-432c-a168-8c435c85623a"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4788e6e5-433d-4d08-b602-71b5fde3b8ba",
      "metadata": {
        "id": "4788e6e5-433d-4d08-b602-71b5fde3b8ba"
      },
      "outputs": [],
      "source": [
        "MAX_QUESTION_LENGTH = 50\n",
        "MAX_ANSWER_LENGTH = 200\n",
        "EMBEDDING_DIM = 384\n",
        "LATENT_DIM = 768\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "VALIDATION_SPLIT = 0.2\n",
        "TEST_SPLIT = 0.2\n",
        "LEARNING_RATE = 5e-5\n",
        "CLIP_NORM = 1.0\n",
        "BEAM_WIDTH = 3\n",
        "TEMPERATURE = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2e0975c0-ad56-41cd-a7d2-b41c677c4160",
      "metadata": {
        "id": "2e0975c0-ad56-41cd-a7d2-b41c677c4160"
      },
      "outputs": [],
      "source": [
        "def augment_data(question, answer):\n",
        "    augmented_pairs = []\n",
        "    # Original pair\n",
        "    augmented_pairs.append((question, answer))\n",
        "\n",
        "    # Remove punctuation version\n",
        "    q_no_punct = re.sub(r'[.,!?]', '', question)\n",
        "    augmented_pairs.append((q_no_punct, answer))\n",
        "\n",
        "    # Shuffle words slightly (maintaining rough meaning)\n",
        "    words = question.split()\n",
        "    if len(words) > 3:\n",
        "        for i in range(min(3, len(words)-1)):\n",
        "            shuffled = words.copy()\n",
        "            shuffled[i], shuffled[i+1] = shuffled[i+1], shuffled[i]\n",
        "            augmented_pairs.append((' '.join(shuffled), answer))\n",
        "\n",
        "    return augmented_pairs\n",
        "\n",
        "def get_synonyms(word):\n",
        "    try:\n",
        "        synonyms = []\n",
        "        for syn in wordnet.synsets(word):\n",
        "            for lemma in syn.lemmas():\n",
        "                if lemma.name() != word and '_' not in lemma.name():\n",
        "                    synonyms.append(lemma.name())\n",
        "        return list(set(synonyms))\n",
        "    except LookupError:\n",
        "        nltk.download('wordnet')\n",
        "        return get_synonyms(word)\n",
        "    except Exception as e:\n",
        "        print(f\"Synonym error: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def advanced_augment_data(question, answer, augmentation_factor=3):\n",
        "    \"\"\"\n",
        "    Simplified augmentation without async operations\n",
        "    \"\"\"\n",
        "    augmented_pairs = [(question, answer)]\n",
        "\n",
        "    # 1. Synonym replacement\n",
        "    words = question.split()\n",
        "    for _ in range(min(3, len(words))):\n",
        "        new_words = words.copy()\n",
        "        idx = random.randint(0, len(words)-1)\n",
        "        synonyms = get_synonyms(words[idx])\n",
        "        if synonyms:\n",
        "            new_words[idx] = random.choice(synonyms)\n",
        "            augmented_pairs.append((' '.join(new_words), answer))\n",
        "\n",
        "    # 2. Random deletion\n",
        "    if len(words) > 4:\n",
        "        new_words = [word for word in words if random.random() > 0.2]\n",
        "        if new_words:\n",
        "            augmented_pairs.append((' '.join(new_words), answer))\n",
        "\n",
        "    # 3. Word order variations\n",
        "    if len(augmented_pairs) < augmentation_factor:\n",
        "        if len(words) > 3:\n",
        "            for i in range(min(2, len(words)-1)):\n",
        "                shuffled = words.copy()\n",
        "                shuffled[i], shuffled[i+1] = shuffled[i+1], shuffled[i]\n",
        "                augmented_pairs.append((' '.join(shuffled), answer))\n",
        "\n",
        "    # 4. Remove punctuation version\n",
        "    q_no_punct = re.sub(r'[.,!?]', '', question)\n",
        "    if q_no_punct != question:\n",
        "        augmented_pairs.append((q_no_punct, answer))\n",
        "\n",
        "    return augmented_pairs[:augmentation_factor]\n",
        "\n",
        "def load_data(dataset_path):\n",
        "    data = []\n",
        "    for file in sorted(os.listdir(dataset_path)):\n",
        "        if file.endswith(\".json\"):\n",
        "            with open(os.path.join(dataset_path, file), \"r\", encoding=\"utf-8\") as f:\n",
        "                content = json.load(f)\n",
        "                qa_pairs = content.get(\"qa_pairs\", [])\n",
        "                for pair in qa_pairs:\n",
        "                    augmented = augment_data(pair[\"question\"], pair[\"answer\"])\n",
        "                    for q, a in augmented:\n",
        "                        data.append({\"question\": q, \"answer\": a})\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def improved_clean_text(text):\n",
        "    \"\"\"Enhanced text cleaning with better special character handling\"\"\"\n",
        "    text = text.lower().strip()\n",
        "    # Preserve more meaningful punctuation and symbols\n",
        "    text = re.sub(r'[^\\w\\s.,!?\\'\"-:;$%#@&*()]', ' ', text)\n",
        "    # Normalize numbers\n",
        "    text = re.sub(r'\\d+', 'NUM', text)\n",
        "    # Normalize spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return f\"<START> {text} <END>\"\n",
        "\n",
        "def preprocess_data(df):\n",
        "    df[\"question\"] = df[\"question\"].apply(improved_clean_text)\n",
        "    df[\"answer\"] = df[\"answer\"].apply(improved_clean_text)\n",
        "\n",
        "    # Create tokenizers with additional special tokens\n",
        "    question_tokenizer = Tokenizer(oov_token=\"<UNK>\", filters='')\n",
        "    answer_tokenizer = Tokenizer(oov_token=\"<UNK>\", filters='')\n",
        "\n",
        "    # Add padding token\n",
        "    question_tokenizer.word_index['<PAD>'] = 0\n",
        "    answer_tokenizer.word_index['<PAD>'] = 0\n",
        "\n",
        "    # Fit tokenizers\n",
        "    question_tokenizer.fit_on_texts(df[\"question\"])\n",
        "    answer_tokenizer.fit_on_texts(df[\"answer\"])\n",
        "\n",
        "    # Convert to sequences\n",
        "    question_sequences = question_tokenizer.texts_to_sequences(df[\"question\"])\n",
        "    answer_sequences = answer_tokenizer.texts_to_sequences(df[\"answer\"])\n",
        "\n",
        "    # Pad sequences\n",
        "    question_padded = pad_sequences(question_sequences, maxlen=MAX_QUESTION_LENGTH, padding='post')\n",
        "    answer_padded = pad_sequences(answer_sequences, maxlen=MAX_ANSWER_LENGTH, padding='post')\n",
        "\n",
        "    return question_padded, answer_padded, question_tokenizer, answer_tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0973cf5d-6355-41b9-8c4e-50d8457ec5ed",
      "metadata": {
        "id": "0973cf5d-6355-41b9-8c4e-50d8457ec5ed"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "1b0ec4b8-0012-4718-8e09-fb2123f7adc6",
      "metadata": {
        "id": "1b0ec4b8-0012-4718-8e09-fb2123f7adc6"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(Layer):\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super(BahdanauAttention, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W1 = self.add_weight(\n",
        "            shape=(input_shape[0][-1], self.units),\n",
        "            initializer='glorot_uniform',\n",
        "            name='W1'\n",
        "        )\n",
        "        self.W2 = self.add_weight(\n",
        "            shape=(input_shape[1][-1], self.units),\n",
        "            initializer='glorot_uniform',\n",
        "            name='W2'\n",
        "        )\n",
        "        self.V = self.add_weight(\n",
        "            shape=(self.units, 1),\n",
        "            initializer='glorot_uniform',\n",
        "            name='V'\n",
        "        )\n",
        "        super(BahdanauAttention, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        decoder_hidden_states, encoder_outputs = inputs\n",
        "\n",
        "        # Transform states\n",
        "        decoder_hidden = tf.tensordot(decoder_hidden_states, self.W1, axes=[[2], [0]])\n",
        "        encoder_hidden = tf.tensordot(encoder_outputs, self.W2, axes=[[2], [0]])\n",
        "\n",
        "        # Expand dimensions for broadcasting\n",
        "        decoder_hidden_expanded = tf.expand_dims(decoder_hidden, 2)\n",
        "        encoder_hidden_expanded = tf.expand_dims(encoder_hidden, 1)\n",
        "\n",
        "        # Calculate attention\n",
        "        tanh_output = tf.tanh(decoder_hidden_expanded + encoder_hidden_expanded)\n",
        "        attention_weights = tf.tensordot(tanh_output, self.V, axes=[[3], [0]])\n",
        "        attention_weights = tf.squeeze(attention_weights, axis=-1)\n",
        "        attention_weights = tf.nn.softmax(attention_weights, axis=2)\n",
        "\n",
        "        # Apply attention weights\n",
        "        context_vector = tf.matmul(attention_weights, encoder_outputs)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(BahdanauAttention, self).get_config()\n",
        "        config.update({\n",
        "            'units': self.units\n",
        "        })\n",
        "        return config\n",
        "\n",
        "class AttentionDecoderLayer(Layer):\n",
        "    def __init__(self, latent_dim, vocab_size, **kwargs):\n",
        "        super(AttentionDecoderLayer, self).__init__(**kwargs)\n",
        "        self.latent_dim = latent_dim\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Forward and backward LSTMs\n",
        "        self.forward_lstm = LSTM(\n",
        "            latent_dim,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            recurrent_initializer='glorot_uniform',\n",
        "            name='forward_lstm'\n",
        "        )\n",
        "\n",
        "        self.backward_lstm = LSTM(\n",
        "            latent_dim,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            recurrent_initializer='glorot_uniform',\n",
        "            go_backwards=True,\n",
        "            name='backward_lstm'\n",
        "        )\n",
        "\n",
        "        # Create layers with proper naming\n",
        "        self.attention = BahdanauAttention(latent_dim)\n",
        "        self.concat = Concatenate()\n",
        "        self.dense1 = Dense(latent_dim * 2, activation='relu')\n",
        "        self.dropout = Dropout(0.3)\n",
        "        self.dense2 = Dense(vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=None):\n",
        "        decoder_inputs, encoder_outputs = inputs\n",
        "\n",
        "        if states is None:\n",
        "            # Initialize states with zeros if not provided\n",
        "            batch_size = tf.shape(decoder_inputs)[0]\n",
        "            initial_state_h = tf.zeros((batch_size, self.latent_dim))\n",
        "            initial_state_c = tf.zeros((batch_size, self.latent_dim))\n",
        "            states = [initial_state_h, initial_state_c, initial_state_h, initial_state_c]\n",
        "\n",
        "        # Split states for forward and backward LSTM\n",
        "        forward_h, forward_c, backward_h, backward_c = states\n",
        "\n",
        "        # Process forward LSTM\n",
        "        forward_outputs, forward_h_new, forward_c_new = self.forward_lstm(\n",
        "            decoder_inputs,\n",
        "            initial_state=[forward_h, forward_c]\n",
        "        )\n",
        "\n",
        "        # Process backward LSTM\n",
        "        backward_outputs, backward_h_new, backward_c_new = self.backward_lstm(\n",
        "            decoder_inputs,\n",
        "            initial_state=[backward_h, backward_c]\n",
        "        )\n",
        "\n",
        "        # Combine bidirectional outputs\n",
        "        decoder_outputs = self.concat([forward_outputs, backward_outputs])\n",
        "\n",
        "        # Apply attention\n",
        "        context_vector, _ = self.attention([decoder_outputs, encoder_outputs])\n",
        "\n",
        "        # Combine context with decoder outputs\n",
        "        decoder_combined = self.concat([decoder_outputs, context_vector])\n",
        "\n",
        "        # Process through dense layers\n",
        "        outputs = self.dense1(decoder_combined)\n",
        "        outputs = self.dropout(outputs, training=training)\n",
        "        outputs = self.dense2(outputs)\n",
        "\n",
        "        if return_state:\n",
        "            new_states = [forward_h_new, forward_c_new, backward_h_new, backward_c_new]\n",
        "            return outputs, new_states\n",
        "        return outputs\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(AttentionDecoderLayer, self).get_config()\n",
        "        config.update({\n",
        "            'latent_dim': self.latent_dim,\n",
        "            'vocab_size': self.vocab_size\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "def build_improved_model(vocab_size_q, vocab_size_a):\n",
        "    \"\"\"Build the main training model with consistent layer naming\"\"\"\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(MAX_QUESTION_LENGTH,), dtype=tf.float32, name='encoder_inputs')\n",
        "\n",
        "    encoder_embedding = Embedding(\n",
        "        vocab_size_q,\n",
        "        EMBEDDING_DIM,\n",
        "        mask_zero=True,\n",
        "        name='encoder_embedding'\n",
        "    )(encoder_inputs)\n",
        "\n",
        "    encoder_dropout = Dropout(0.3, name='encoder_dropout')(encoder_embedding)\n",
        "\n",
        "    # Bidirectional encoder - explicit naming\n",
        "    encoder_bilstm = Bidirectional(\n",
        "        LSTM(\n",
        "            LATENT_DIM,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            recurrent_initializer='glorot_uniform'\n",
        "        ),\n",
        "        name='encoder_bilstm'\n",
        "    )\n",
        "\n",
        "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_bilstm(encoder_dropout)\n",
        "\n",
        "    # Combine states\n",
        "    state_h = Concatenate(name='encoder_state_h_concat')([forward_h, backward_h])\n",
        "    state_c = Concatenate(name='encoder_state_c_concat')([forward_c, backward_c])\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(None,), dtype=tf.float32, name='decoder_inputs')\n",
        "\n",
        "    decoder_embedding = Embedding(\n",
        "        vocab_size_a,\n",
        "        EMBEDDING_DIM,\n",
        "        mask_zero=True,\n",
        "        name='decoder_embedding'\n",
        "    )(decoder_inputs)\n",
        "\n",
        "    decoder_dropout = Dropout(0.3, name='decoder_dropout')(decoder_embedding)\n",
        "\n",
        "    # Create and apply decoder with explicit naming\n",
        "    decoder_layer = AttentionDecoderLayer(LATENT_DIM, vocab_size_a, name='attention_decoder_layer')\n",
        "    decoder_outputs = decoder_layer(\n",
        "        [decoder_dropout, encoder_outputs],\n",
        "        states=[forward_h, forward_c, backward_h, backward_c]\n",
        "    )\n",
        "\n",
        "    # Create model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    # Compile\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        clipnorm=CLIP_NORM\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "93adade3-ddda-4ec0-bd7f-680bf2632f2f",
      "metadata": {
        "id": "93adade3-ddda-4ec0-bd7f-680bf2632f2f"
      },
      "outputs": [],
      "source": [
        "def create_inference_models(model, vocab_size_a):\n",
        "    \"\"\"Create encoder and decoder models for inference\"\"\"\n",
        "\n",
        "    # Encoder\n",
        "    encoder_inputs = model.input[0]  # Get encoder inputs from training model\n",
        "\n",
        "    # Get encoder layers\n",
        "    encoder_embedding = model.get_layer('encoder_embedding')\n",
        "    encoder_lstm = model.get_layer('encoder_bilstm')  # Get the bidirectional layer\n",
        "\n",
        "    # Create encoder model\n",
        "    encoder_embedded = encoder_embedding(encoder_inputs)\n",
        "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_embedded)\n",
        "\n",
        "    # Combine states\n",
        "    state_h = Concatenate()([forward_h, backward_h])\n",
        "    state_c = Concatenate()([forward_c, backward_c])\n",
        "\n",
        "    encoder_model = Model(\n",
        "        inputs=encoder_inputs,\n",
        "        outputs=[encoder_outputs, state_h, state_c]\n",
        "    )\n",
        "\n",
        "    # Decoder setup for inference\n",
        "    decoder_inputs = Input(shape=(1,))\n",
        "    encoder_states_inputs = [\n",
        "        Input(shape=(MAX_QUESTION_LENGTH, LATENT_DIM * 2)),  # encoder_outputs\n",
        "        Input(shape=(LATENT_DIM * 2,)),  # state_h\n",
        "        Input(shape=(LATENT_DIM * 2,))   # state_c\n",
        "    ]\n",
        "\n",
        "    # Get decoder layers\n",
        "    decoder_embedding = model.get_layer('decoder_embedding')\n",
        "    decoder_lstm = model.get_layer('attention_decoder_layer')\n",
        "\n",
        "    # Process through decoder\n",
        "    decoder_embedded = decoder_embedding(decoder_inputs)\n",
        "\n",
        "    # Split states for bidirectional decoder\n",
        "    decoder_lstm_dim = LATENT_DIM\n",
        "    states_h = encoder_states_inputs[1]\n",
        "    states_c = encoder_states_inputs[2]\n",
        "\n",
        "    forward_h = Lambda(lambda x: x[:, :decoder_lstm_dim])(states_h)\n",
        "    forward_c = Lambda(lambda x: x[:, :decoder_lstm_dim])(states_c)\n",
        "    backward_h = Lambda(lambda x: x[:, decoder_lstm_dim:])(states_h)\n",
        "    backward_c = Lambda(lambda x: x[:, decoder_lstm_dim:])(states_c)\n",
        "\n",
        "    decoder_outputs = decoder_lstm(\n",
        "        [decoder_embedded, encoder_states_inputs[0]],\n",
        "        states=[forward_h, forward_c, backward_h, backward_c],\n",
        "        training=False\n",
        "    )\n",
        "\n",
        "    decoder_model = Model(\n",
        "        inputs=[decoder_inputs] + encoder_states_inputs,\n",
        "        outputs=decoder_outputs\n",
        "    )\n",
        "\n",
        "    return encoder_model, decoder_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "fdfc2158-b836-4d5a-ab15-cb905746eaa9",
      "metadata": {
        "id": "fdfc2158-b836-4d5a-ab15-cb905746eaa9"
      },
      "outputs": [],
      "source": [
        "def optimized_beam_search_decode(encoder_model, decoder_model, input_seq, a_tokenizer,\n",
        "                               beam_width=3, max_length=50, temperature=1.0,\n",
        "                               early_stopping_threshold=0.001):\n",
        "    \"\"\"\n",
        "    Optimized beam search decoding with fixed tensor shapes\n",
        "    \"\"\"\n",
        "    # Ensure input_seq is a tensor with correct shape\n",
        "    input_seq = tf.convert_to_tensor(input_seq)\n",
        "    if len(input_seq.shape) == 1:\n",
        "        input_seq = tf.expand_dims(input_seq, 0)\n",
        "\n",
        "    # Get encoder outputs\n",
        "    encoder_outputs, state_h, state_c = encoder_model(input_seq, training=False)\n",
        "\n",
        "    # Initialize beam search\n",
        "    start_token = a_tokenizer.word_index['<start>']\n",
        "    end_token = a_tokenizer.word_index['<end>']\n",
        "\n",
        "    initial_beam = ([start_token], 0.0)\n",
        "    beams = [(initial_beam, (encoder_outputs, state_h, state_c))]\n",
        "    finished_beams = []\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        candidates = []\n",
        "\n",
        "        for (seq, score), (enc_out, h, c) in beams:\n",
        "            if seq[-1] == end_token:\n",
        "                finished_beams.append((seq, score))\n",
        "                continue\n",
        "\n",
        "            # Prepare decoder input\n",
        "            target_seq = tf.convert_to_tensor([[seq[-1]]], dtype=tf.float32)\n",
        "\n",
        "            # Decoder step\n",
        "            decoder_outputs = decoder_model(\n",
        "                [target_seq, enc_out, h, c],\n",
        "                training=False\n",
        "            )\n",
        "\n",
        "            # Get probabilities\n",
        "            logits = decoder_outputs[0, -1, :]\n",
        "            probs = tf.nn.softmax(logits / temperature).numpy()\n",
        "\n",
        "            # Get top k candidates\n",
        "            top_k_probs, top_k_indices = tf.nn.top_k(probs, k=beam_width)\n",
        "\n",
        "            for prob, idx in zip(top_k_probs.numpy(), top_k_indices.numpy()):\n",
        "                new_seq = seq + [idx]\n",
        "                new_score = score + float(tf.math.log(prob + 1e-10))\n",
        "                candidates.append(\n",
        "                    ((new_seq, new_score), (enc_out, h, c))\n",
        "                )\n",
        "\n",
        "        # Early stopping check\n",
        "        if finished_beams:\n",
        "            best_finished_score = max(score for _, score in finished_beams)\n",
        "            candidates = [c for c in candidates\n",
        "                        if c[0][1] + early_stopping_threshold >= best_finished_score]\n",
        "\n",
        "        # Select top beams\n",
        "        beams = sorted(candidates, key=lambda x: x[0][1], reverse=True)[:beam_width]\n",
        "\n",
        "        # Break if all beams finished\n",
        "        if all(beam[0][0][-1] == end_token for beam in beams):\n",
        "            break\n",
        "\n",
        "    # Get best sequence\n",
        "    if finished_beams:\n",
        "        best_seq = max(finished_beams, key=lambda x: x[1])[0]\n",
        "    else:\n",
        "        best_seq = max(beams, key=lambda x: x[0][1])[0][0]\n",
        "\n",
        "    return best_seq  # Return the highest scoring sequence\n",
        "\n",
        "# Training history plotting\n",
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Plot training metrics and learning rate\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Loss plot\n",
        "    axes[0, 0].plot(history.history['loss'], label='Training Loss')\n",
        "    axes[0, 0].plot(history.history['val_loss'], label='Validation Loss')\n",
        "    axes[0, 0].set_title('Model Loss')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "\n",
        "    # Accuracy plot\n",
        "    axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    axes[0, 1].set_title('Model Accuracy')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Accuracy')\n",
        "    axes[0, 1].legend()\n",
        "\n",
        "    # Learning rate plot\n",
        "    axes[1, 0].plot(history.history['learning_rate'], label='Learning Rate')\n",
        "    axes[1, 0].set_title('Learning Rate')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Learning Rate')\n",
        "    axes[1, 0].set_yscale('log')\n",
        "\n",
        "    # Clean up and display\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1cf8d4d6-f5dd-4118-a53a-7c3cd3b53514",
      "metadata": {
        "id": "1cf8d4d6-f5dd-4118-a53a-7c3cd3b53514"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_advanced(encoder_model, decoder_model, q_test, a_test, q_tokenizer, a_tokenizer,\n",
        "                          beam_width=3, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Enhanced model evaluation with optimized beam search\n",
        "    \"\"\"\n",
        "    smooth = SmoothingFunction().method1\n",
        "    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n",
        "\n",
        "    # Initialize metrics\n",
        "    bleu_scores = {f'bleu_{i}': 0.0 for i in range(1, 5)}\n",
        "    rouge_scores = {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
        "\n",
        "    # Convert test data to tensors once\n",
        "    q_test = tf.convert_to_tensor(q_test, dtype=tf.float32)\n",
        "\n",
        "    # Process in batches for efficiency\n",
        "    batch_size = 32\n",
        "    for i in range(0, len(q_test), batch_size):\n",
        "        batch_end = min(i + batch_size, len(q_test))\n",
        "        batch_q = q_test[i:batch_end]\n",
        "\n",
        "        references = []\n",
        "        hypotheses = []\n",
        "\n",
        "        for j in range(batch_end - i):\n",
        "            # Get reference tokens\n",
        "            reference_tokens = [a_tokenizer.index_word.get(idx, '')\n",
        "                              for idx in a_test[i + j] if idx != 0]\n",
        "            reference_tokens = [token for token in reference_tokens\n",
        "                              if token not in ['<start>', '<end>', '<pad>']]\n",
        "            references.append([reference_tokens])\n",
        "\n",
        "            # Generate response using optimized beam search\n",
        "            decoded_sequence = optimized_beam_search_decode(\n",
        "                encoder_model,\n",
        "                decoder_model,\n",
        "                batch_q[j:j+1],\n",
        "                a_tokenizer,\n",
        "                beam_width=beam_width,\n",
        "                temperature=temperature\n",
        "            )\n",
        "\n",
        "            decoded_tokens = [a_tokenizer.index_word.get(idx, '')\n",
        "                            for idx in decoded_sequence\n",
        "                            if idx not in [a_tokenizer.word_index.get(t, 0)\n",
        "                                         for t in ['<start>', '<end>', '<pad>']]]\n",
        "            hypotheses.append(decoded_tokens)\n",
        "\n",
        "            # Calculate ROUGE scores\n",
        "            rouge_scores_i = rouge_scorer_instance.score(\n",
        "                ' '.join(reference_tokens),\n",
        "                ' '.join(decoded_tokens)\n",
        "            )\n",
        "            for key in rouge_scores:\n",
        "                rouge_scores[key] += rouge_scores_i[key].fmeasure\n",
        "\n",
        "        # Update BLEU scores for batch\n",
        "        for i in range(1, 5):\n",
        "            bleu_scores[f'bleu_{i}'] += corpus_bleu(\n",
        "                references,\n",
        "                hypotheses,\n",
        "                weights=[1.0/i]*i,\n",
        "                smoothing_function=smooth\n",
        "            ) * len(references)\n",
        "\n",
        "    # Normalize scores\n",
        "    n = len(q_test)\n",
        "    bleu_scores = {k: v/n for k, v in bleu_scores.items()}\n",
        "    rouge_scores = {k: v/n for k, v in rouge_scores.items()}\n",
        "\n",
        "    return bleu_scores, rouge_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "7dd83b1b-ed9b-4ed3-84e3-29340384f015",
      "metadata": {
        "id": "7dd83b1b-ed9b-4ed3-84e3-29340384f015"
      },
      "outputs": [],
      "source": [
        "def ask_question(question, encoder_model, decoder_model, q_tokenizer, a_tokenizer, max_length=50):\n",
        "    \"\"\"Ask a question to the trained model and get its response\"\"\"\n",
        "    # Preprocess question\n",
        "    question = improved_clean_text(question)\n",
        "    q_seq = q_tokenizer.texts_to_sequences([question])\n",
        "    q_seq = pad_sequences(q_seq, maxlen=MAX_QUESTION_LENGTH, padding='post')\n",
        "\n",
        "    # Convert to tensor\n",
        "    q_seq = tf.convert_to_tensor(q_seq, dtype=tf.float32)\n",
        "\n",
        "    # Get encoder outputs\n",
        "    encoder_outputs, state_h, state_c = encoder_model(q_seq, training=False)\n",
        "\n",
        "    # Initialize decoder sequence\n",
        "    target_seq = tf.convert_to_tensor([[a_tokenizer.word_index['<start>']]], dtype=tf.float32)\n",
        "\n",
        "    decoded_tokens = []\n",
        "\n",
        "    while len(decoded_tokens) < max_length:\n",
        "        # Get decoder outputs\n",
        "        decoder_outputs = decoder_model(\n",
        "            [target_seq, encoder_outputs, state_h, state_c],\n",
        "            training=False\n",
        "        )\n",
        "\n",
        "        # Sample token\n",
        "        sampled_token_index = tf.argmax(decoder_outputs[0, -1, :]).numpy()\n",
        "        sampled_word = a_tokenizer.index_word.get(sampled_token_index, '')\n",
        "\n",
        "        if sampled_word == '<end>' or sampled_word == '':\n",
        "            break\n",
        "\n",
        "        decoded_tokens.append(sampled_word)\n",
        "\n",
        "        # Update decoder input\n",
        "        target_seq = tf.convert_to_tensor([[sampled_token_index]], dtype=tf.float32)\n",
        "\n",
        "    return ' '.join(decoded_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a5a13493-0b9b-46cf-a678-540895785a67",
      "metadata": {
        "id": "a5a13493-0b9b-46cf-a678-540895785a67"
      },
      "outputs": [],
      "source": [
        "class WarmUpLearningRateScheduler(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, warmup_steps, initial_lr):\n",
        "        super().__init__()\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.initial_lr = initial_lr\n",
        "        self.step = 0\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        self.step += 1\n",
        "        if self.step <= self.warmup_steps:\n",
        "            lr = (self.step / self.warmup_steps) * self.initial_lr\n",
        "            self.model.optimizer.learning_rate.assign(lr)\n",
        "\n",
        "callbacks = [\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True,\n",
        "            min_delta=1e-4\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.2,\n",
        "            patience=10,\n",
        "            min_lr=1e-6,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            'best_model.keras',\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        WarmUpLearningRateScheduler(warmup_steps=100, initial_lr=LEARNING_RATE),\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad19a64f-74b9-439f-bc22-5cd2367d479d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ad19a64f-74b9-439f-bc22-5cd2367d479d",
        "outputId": "be29f1d0-b6bb-4585-b51a-dda1de03a1e9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading and preprocessing data...\n",
            "Original Dataset:\n",
            "\n",
            "                         question  \\\n",
            "0  What is blockchain technology?   \n",
            "1   What is blockchain technology   \n",
            "2  is What blockchain technology?   \n",
            "3  What blockchain is technology?   \n",
            "4  What is technology? blockchain   \n",
            "\n",
            "                                              answer  \n",
            "0  Blockchain is a distributed, decentralized led...  \n",
            "1  Blockchain is a distributed, decentralized led...  \n",
            "2  Blockchain is a distributed, decentralized led...  \n",
            "3  Blockchain is a distributed, decentralized led...  \n",
            "4  Blockchain is a distributed, decentralized led...  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3238 entries, 0 to 3237\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   question  3238 non-null   object\n",
            " 1   answer    3238 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 50.7+ KB\n",
            "None\n",
            "Dataset after preprocessing and augmentation:\n",
            "\n",
            "                                            question  \\\n",
            "0       <START> what is blockchain technology? <END>   \n",
            "1     <START> what live blockchain technology? <END>   \n",
            "2       <START> is what blockchain technology? <END>   \n",
            "3        <START> what is blockchain technology <END>   \n",
            "4  <START> what constitute blockchain technology ...   \n",
            "\n",
            "                                              answer  \n",
            "0  <START> blockchain is a distributed, decentral...  \n",
            "1  <START> blockchain is a distributed, decentral...  \n",
            "2  <START> blockchain is a distributed, decentral...  \n",
            "3  <START> blockchain is a distributed, decentral...  \n",
            "4  <START> blockchain is a distributed, decentral...  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 9707 entries, 0 to 9706\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   question  9707 non-null   object\n",
            " 1   answer    9707 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 151.8+ KB\n",
            "None\n",
            "Building model...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'attention_decoder_layer' (of type AttentionDecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">948,096</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,431,488</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_bilstm            │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>),     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,084,032</span> │ encoder_dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │                │ not_equal_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>),     │                │                        │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)]           │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention_decoder_layer   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6332</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">23,896,508</span> │ decoder_dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionDecoderLayer</span>)   │                        │                │ encoder_bilstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                           │                        │                │ encoder_bilstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],  │\n",
              "│                           │                        │                │ encoder_bilstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>],  │\n",
              "│                           │                        │                │ encoder_bilstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>],  │\n",
              "│                           │                        │                │ encoder_bilstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>]   │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m384\u001b[0m)        │        \u001b[38;5;34m948,096\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)      │      \u001b[38;5;34m2,431,488\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_dropout (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m384\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ encoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal_18 (\u001b[38;5;33mNotEqual\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_dropout (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ decoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_bilstm            │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1536\u001b[0m),     │      \u001b[38;5;34m7,084,032\u001b[0m │ encoder_dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │                │ not_equal_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│                           │ \u001b[38;5;34m768\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m),     │                │                        │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)]           │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention_decoder_layer   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6332\u001b[0m)     │     \u001b[38;5;34m23,896,508\u001b[0m │ decoder_dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
              "│ (\u001b[38;5;33mAttentionDecoderLayer\u001b[0m)   │                        │                │ encoder_bilstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                           │                        │                │ encoder_bilstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],  │\n",
              "│                           │                        │                │ encoder_bilstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m],  │\n",
              "│                           │                        │                │ encoder_bilstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m3\u001b[0m],  │\n",
              "│                           │                        │                │ encoder_bilstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m4\u001b[0m]   │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,360,124</span> (131.07 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m34,360,124\u001b[0m (131.07 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,360,124</span> (131.07 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,360,124\u001b[0m (131.07 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Training model...\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'bahdanau_attention_7' (of type BahdanauAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779ms/step - accuracy: 0.5403 - loss: 6.2767\n",
            "Epoch 1: val_loss improved from 2.80358 to 2.80308, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 957ms/step - accuracy: 0.5409 - loss: 6.2565 - val_accuracy: 0.6321 - val_loss: 2.8031 - learning_rate: 5.0000e-05\n",
            "Epoch 2/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810ms/step - accuracy: 0.6256 - loss: 2.8251\n",
            "Epoch 2: val_loss improved from 2.80308 to 2.74889, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 1s/step - accuracy: 0.6256 - loss: 2.8249 - val_accuracy: 0.6327 - val_loss: 2.7489 - learning_rate: 5.0000e-05\n",
            "Epoch 3/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800ms/step - accuracy: 0.6340 - loss: 2.7428\n",
            "Epoch 3: val_loss improved from 2.74889 to 2.70023, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 986ms/step - accuracy: 0.6339 - loss: 2.7430 - val_accuracy: 0.6368 - val_loss: 2.7002 - learning_rate: 5.0000e-05\n",
            "Epoch 4/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797ms/step - accuracy: 0.6354 - loss: 2.7110\n",
            "Epoch 4: val_loss improved from 2.70023 to 2.67220, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 976ms/step - accuracy: 0.6354 - loss: 2.7111 - val_accuracy: 0.6354 - val_loss: 2.6722 - learning_rate: 5.0000e-05\n",
            "Epoch 5/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 795ms/step - accuracy: 0.6311 - loss: 2.7128\n",
            "Epoch 5: val_loss improved from 2.67220 to 2.63869, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 968ms/step - accuracy: 0.6312 - loss: 2.7124 - val_accuracy: 0.6385 - val_loss: 2.6387 - learning_rate: 5.0000e-05\n",
            "Epoch 6/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796ms/step - accuracy: 0.6357 - loss: 2.6501\n",
            "Epoch 6: val_loss improved from 2.63869 to 2.60175, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 1s/step - accuracy: 0.6357 - loss: 2.6501 - val_accuracy: 0.6404 - val_loss: 2.6018 - learning_rate: 5.0000e-05\n",
            "Epoch 7/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793ms/step - accuracy: 0.6358 - loss: 2.6376\n",
            "Epoch 7: val_loss improved from 2.60175 to 2.57154, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 963ms/step - accuracy: 0.6358 - loss: 2.6373 - val_accuracy: 0.6424 - val_loss: 2.5715 - learning_rate: 5.0000e-05\n",
            "Epoch 8/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813ms/step - accuracy: 0.6385 - loss: 2.5969\n",
            "Epoch 8: val_loss improved from 2.57154 to 2.52365, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 984ms/step - accuracy: 0.6385 - loss: 2.5967 - val_accuracy: 0.6438 - val_loss: 2.5236 - learning_rate: 5.0000e-05\n",
            "Epoch 9/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798ms/step - accuracy: 0.6402 - loss: 2.5506\n",
            "Epoch 9: val_loss improved from 2.52365 to 2.49331, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 1s/step - accuracy: 0.6402 - loss: 2.5505 - val_accuracy: 0.6448 - val_loss: 2.4933 - learning_rate: 5.0000e-05\n",
            "Epoch 10/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796ms/step - accuracy: 0.6406 - loss: 2.5140\n",
            "Epoch 10: val_loss improved from 2.49331 to 2.43171, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 981ms/step - accuracy: 0.6406 - loss: 2.5138 - val_accuracy: 0.6462 - val_loss: 2.4317 - learning_rate: 5.0000e-05\n",
            "Epoch 11/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861ms/step - accuracy: 0.6451 - loss: 2.4393\n",
            "Epoch 11: val_loss improved from 2.43171 to 2.38616, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 1s/step - accuracy: 0.6451 - loss: 2.4394 - val_accuracy: 0.6477 - val_loss: 2.3862 - learning_rate: 5.0000e-05\n",
            "Epoch 12/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862ms/step - accuracy: 0.6431 - loss: 2.4166\n",
            "Epoch 12: val_loss improved from 2.38616 to 2.33564, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 1s/step - accuracy: 0.6431 - loss: 2.4164 - val_accuracy: 0.6494 - val_loss: 2.3356 - learning_rate: 5.0000e-05\n",
            "Epoch 13/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861ms/step - accuracy: 0.6488 - loss: 2.3350\n",
            "Epoch 13: val_loss improved from 2.33564 to 2.28313, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 1s/step - accuracy: 0.6487 - loss: 2.3352 - val_accuracy: 0.6514 - val_loss: 2.2831 - learning_rate: 5.0000e-05\n",
            "Epoch 14/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861ms/step - accuracy: 0.6494 - loss: 2.2915\n",
            "Epoch 14: val_loss improved from 2.28313 to 2.24211, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - accuracy: 0.6494 - loss: 2.2915 - val_accuracy: 0.6516 - val_loss: 2.2421 - learning_rate: 5.0000e-05\n",
            "Epoch 15/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855ms/step - accuracy: 0.6481 - loss: 2.2561\n",
            "Epoch 15: val_loss improved from 2.24211 to 2.19508, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 1s/step - accuracy: 0.6481 - loss: 2.2560 - val_accuracy: 0.6528 - val_loss: 2.1951 - learning_rate: 5.0000e-05\n",
            "Epoch 16/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862ms/step - accuracy: 0.6485 - loss: 2.2189\n",
            "Epoch 16: val_loss improved from 2.19508 to 2.15571, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.6485 - loss: 2.2187 - val_accuracy: 0.6541 - val_loss: 2.1557 - learning_rate: 5.0000e-05\n",
            "Epoch 17/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861ms/step - accuracy: 0.6508 - loss: 2.1685\n",
            "Epoch 17: val_loss improved from 2.15571 to 2.10144, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 1s/step - accuracy: 0.6508 - loss: 2.1684 - val_accuracy: 0.6564 - val_loss: 2.1014 - learning_rate: 5.0000e-05\n",
            "Epoch 18/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862ms/step - accuracy: 0.6502 - loss: 2.1342\n",
            "Epoch 18: val_loss improved from 2.10144 to 2.05599, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.6502 - loss: 2.1340 - val_accuracy: 0.6577 - val_loss: 2.0560 - learning_rate: 5.0000e-05\n",
            "Epoch 19/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862ms/step - accuracy: 0.6513 - loss: 2.0975\n",
            "Epoch 19: val_loss improved from 2.05599 to 2.02255, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 1s/step - accuracy: 0.6513 - loss: 2.0974 - val_accuracy: 0.6588 - val_loss: 2.0226 - learning_rate: 5.0000e-05\n",
            "Epoch 20/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863ms/step - accuracy: 0.6562 - loss: 2.0432\n",
            "Epoch 20: val_loss improved from 2.02255 to 2.00257, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - accuracy: 0.6562 - loss: 2.0432 - val_accuracy: 0.6594 - val_loss: 2.0026 - learning_rate: 5.0000e-05\n",
            "Epoch 21/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862ms/step - accuracy: 0.6511 - loss: 2.0489\n",
            "Epoch 21: val_loss improved from 2.00257 to 1.95471, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.6511 - loss: 2.0485 - val_accuracy: 0.6613 - val_loss: 1.9547 - learning_rate: 5.0000e-05\n",
            "Epoch 22/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863ms/step - accuracy: 0.6569 - loss: 1.9849\n",
            "Epoch 22: val_loss improved from 1.95471 to 1.92359, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.6569 - loss: 1.9849 - val_accuracy: 0.6623 - val_loss: 1.9236 - learning_rate: 5.0000e-05\n",
            "Epoch 23/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857ms/step - accuracy: 0.6574 - loss: 1.9534\n",
            "Epoch 23: val_loss improved from 1.92359 to 1.89271, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 1s/step - accuracy: 0.6574 - loss: 1.9534 - val_accuracy: 0.6637 - val_loss: 1.8927 - learning_rate: 5.0000e-05\n",
            "Epoch 24/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864ms/step - accuracy: 0.6575 - loss: 1.9331\n",
            "Epoch 24: val_loss improved from 1.89271 to 1.86356, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - accuracy: 0.6575 - loss: 1.9330 - val_accuracy: 0.6651 - val_loss: 1.8636 - learning_rate: 5.0000e-05\n",
            "Epoch 25/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862ms/step - accuracy: 0.6602 - loss: 1.8897\n",
            "Epoch 25: val_loss improved from 1.86356 to 1.83778, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - accuracy: 0.6602 - loss: 1.8897 - val_accuracy: 0.6665 - val_loss: 1.8378 - learning_rate: 5.0000e-05\n",
            "Epoch 26/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862ms/step - accuracy: 0.6595 - loss: 1.8760\n",
            "Epoch 26: val_loss improved from 1.83778 to 1.80516, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.6595 - loss: 1.8759 - val_accuracy: 0.6672 - val_loss: 1.8052 - learning_rate: 5.0000e-05\n",
            "Epoch 27/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859ms/step - accuracy: 0.6614 - loss: 1.8446\n",
            "Epoch 27: val_loss improved from 1.80516 to 1.78439, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 1s/step - accuracy: 0.6614 - loss: 1.8445 - val_accuracy: 0.6686 - val_loss: 1.7844 - learning_rate: 5.0000e-05\n",
            "Epoch 28/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863ms/step - accuracy: 0.6642 - loss: 1.8103\n",
            "Epoch 28: val_loss improved from 1.78439 to 1.75107, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - accuracy: 0.6642 - loss: 1.8103 - val_accuracy: 0.6702 - val_loss: 1.7511 - learning_rate: 5.0000e-05\n",
            "Epoch 29/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863ms/step - accuracy: 0.6628 - loss: 1.7951\n",
            "Epoch 29: val_loss improved from 1.75107 to 1.73624, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 1s/step - accuracy: 0.6628 - loss: 1.7950 - val_accuracy: 0.6714 - val_loss: 1.7362 - learning_rate: 5.0000e-05\n",
            "Epoch 30/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862ms/step - accuracy: 0.6624 - loss: 1.7844\n",
            "Epoch 30: val_loss improved from 1.73624 to 1.69488, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - accuracy: 0.6624 - loss: 1.7842 - val_accuracy: 0.6741 - val_loss: 1.6949 - learning_rate: 5.0000e-05\n",
            "Epoch 31/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862ms/step - accuracy: 0.6667 - loss: 1.7441\n",
            "Epoch 31: val_loss improved from 1.69488 to 1.67574, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.6667 - loss: 1.7440 - val_accuracy: 0.6747 - val_loss: 1.6757 - learning_rate: 5.0000e-05\n",
            "Epoch 32/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858ms/step - accuracy: 0.6668 - loss: 1.7266\n",
            "Epoch 32: val_loss improved from 1.67574 to 1.65213, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 1s/step - accuracy: 0.6669 - loss: 1.7265 - val_accuracy: 0.6766 - val_loss: 1.6521 - learning_rate: 5.0000e-05\n",
            "Epoch 33/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855ms/step - accuracy: 0.6689 - loss: 1.7062\n",
            "Epoch 33: val_loss improved from 1.65213 to 1.63860, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 1s/step - accuracy: 0.6689 - loss: 1.7061 - val_accuracy: 0.6790 - val_loss: 1.6386 - learning_rate: 5.0000e-05\n",
            "Epoch 34/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859ms/step - accuracy: 0.6711 - loss: 1.6750\n",
            "Epoch 34: val_loss improved from 1.63860 to 1.61059, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 1s/step - accuracy: 0.6711 - loss: 1.6749 - val_accuracy: 0.6805 - val_loss: 1.6106 - learning_rate: 5.0000e-05\n",
            "Epoch 35/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863ms/step - accuracy: 0.6751 - loss: 1.6483\n",
            "Epoch 35: val_loss improved from 1.61059 to 1.59221, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - accuracy: 0.6751 - loss: 1.6483 - val_accuracy: 0.6816 - val_loss: 1.5922 - learning_rate: 5.0000e-05\n",
            "Epoch 36/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858ms/step - accuracy: 0.6691 - loss: 1.6676\n",
            "Epoch 36: val_loss improved from 1.59221 to 1.56753, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 1s/step - accuracy: 0.6692 - loss: 1.6674 - val_accuracy: 0.6862 - val_loss: 1.5675 - learning_rate: 5.0000e-05\n",
            "Epoch 37/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863ms/step - accuracy: 0.6756 - loss: 1.6308\n",
            "Epoch 37: val_loss did not improve from 1.56753\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 976ms/step - accuracy: 0.6756 - loss: 1.6307 - val_accuracy: 0.6822 - val_loss: 1.5780 - learning_rate: 5.0000e-05\n",
            "Epoch 38/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863ms/step - accuracy: 0.6768 - loss: 1.6021\n",
            "Epoch 38: val_loss improved from 1.56753 to 1.55793, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 1s/step - accuracy: 0.6769 - loss: 1.6019 - val_accuracy: 0.6872 - val_loss: 1.5579 - learning_rate: 5.0000e-05\n",
            "Epoch 39/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863ms/step - accuracy: 0.6815 - loss: 1.5705\n",
            "Epoch 39: val_loss improved from 1.55793 to 1.52732, saving model to best_model.keras\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 1s/step - accuracy: 0.6815 - loss: 1.5706 - val_accuracy: 0.6879 - val_loss: 1.5273 - learning_rate: 5.0000e-05\n",
            "Epoch 40/100\n",
            "\u001b[1m39/92\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 897ms/step - accuracy: 0.6857 - loss: 1.5376"
          ]
        }
      ],
      "source": [
        "print(\"Loading and preprocessing data...\")\n",
        "# Load data\n",
        "df = load_data(\"dataset\")\n",
        "print(\"Original Dataset:\\n\")\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "\n",
        "# Preprocess with augmentation\n",
        "augmented_data = []\n",
        "for _, row in df.iterrows():\n",
        "    pairs = advanced_augment_data(row['question'], row['answer'])\n",
        "    augmented_data.extend(pairs)\n",
        "\n",
        "df_augmented = pd.DataFrame(augmented_data, columns=['question', 'answer'])\n",
        "\n",
        "# Rest of your preprocessing steps\n",
        "q_data, a_data, q_tokenizer, a_tokenizer = preprocess_data(df_augmented)\n",
        "print(\"Dataset after preprocessing and augmentation:\\n\")\n",
        "print(df_augmented.head())\n",
        "print(df_augmented.info())\n",
        "\n",
        "# Split data\n",
        "indices = np.arange(len(q_data))\n",
        "np.random.shuffle(indices)\n",
        "q_data = q_data[indices]\n",
        "a_data = a_data[indices]\n",
        "\n",
        "num_val = int(len(q_data) * VALIDATION_SPLIT)\n",
        "num_test = int(len(q_data) * TEST_SPLIT)\n",
        "\n",
        "q_train = q_data[:-num_val-num_test]\n",
        "a_train = a_data[:-num_val-num_test]\n",
        "q_val = q_data[-num_val-num_test:-num_test]\n",
        "a_val = a_data[-num_val-num_test:-num_test]\n",
        "q_test = q_data[-num_test:]\n",
        "a_test = a_data[-num_test:]\n",
        "\n",
        "print(\"Building model...\")\n",
        "# Build model\n",
        "model = build_improved_model(len(q_tokenizer.word_index) + 1, len(a_tokenizer.word_index) + 1)\n",
        "\n",
        "print(\"Training model...\")\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    [q_train, a_train[:, :-1]],\n",
        "    a_train[:, 1:],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=([q_val, a_val[:, :-1]], a_val[:, 1:]),\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "print(\"Plotting training history...\")\n",
        "plot_training_history(history)\n",
        "\n",
        "# Create inference models\n",
        "print(\"Creating inference models...\")\n",
        "encoder_model, decoder_model = create_inference_models(model, len(a_tokenizer.word_index) + 1)\n",
        "\n",
        "# Save models\n",
        "print(\"Saving models...\")\n",
        "model.save('full_model.keras')\n",
        "encoder_model.save('encoder_model.keras')\n",
        "decoder_model.save('decoder_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49fa035e-740e-4d2b-90cb-9328aecabe33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49fa035e-740e-4d2b-90cb-9328aecabe33",
        "outputId": "aaf6a7c7-ce76-4c63-cf67-9d5dc5de6d33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model...\n",
            "\n",
            "Evaluation Results:\n",
            "BLEU_1: 0.4214\n",
            "BLEU_2: 0.4010\n",
            "BLEU_3: 0.3883\n",
            "BLEU_4: 0.3791\n",
            "ROUGE1: 0.6587\n",
            "ROUGE2: 0.6040\n",
            "ROUGEL: 0.6434\n"
          ]
        }
      ],
      "source": [
        "# Load the encoder and decoder separately\n",
        "encoder_model = load_model('encoder_model.keras')\n",
        "decoder_model = load_model('decoder_model.keras')\n",
        "\n",
        "# Evaluate model\n",
        "print(\"Evaluating model...\")\n",
        "bleu_scores, rouge_scores = evaluate_model_advanced(\n",
        "    encoder_model, decoder_model,\n",
        "    q_test, a_test,\n",
        "    q_tokenizer, a_tokenizer,\n",
        "    beam_width=BEAM_WIDTH,\n",
        "    temperature=TEMPERATURE\n",
        ")\n",
        "\n",
        "# Print results\n",
        "print(\"\\nEvaluation Results:\")\n",
        "for metric, score in bleu_scores.items():\n",
        "    print(f\"{metric.upper()}: {score:.4f}\")\n",
        "for metric, score in rouge_scores.items():\n",
        "    print(f\"{metric.upper()}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G-jwJpg43t3D"
      },
      "id": "G-jwJpg43t3D",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (block-query)",
      "language": "python",
      "name": "block-query"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}