{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project Setup"
      ],
      "metadata": {
        "id": "LzgWRpMNk7QW"
      },
      "id": "LzgWRpMNk7QW"
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/pratyushsharma258/Block-Query.git\n",
        "! cd Block-Query && pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "n_lxUrX2k6EC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19bd348a-2e0f-4b85-b972-2f293ad90b1a"
      },
      "id": "n_lxUrX2k6EC",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Block-Query'...\n",
            "remote: Enumerating objects: 71, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 71 (delta 24), reused 57 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (71/71), 1.04 MiB | 3.09 MiB/s, done.\n",
            "Resolving deltas: 100% (24/24), done.\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (4.67.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (3.9.1)\n",
            "Collecting rouge-score (from -r requirements.txt (line 9))\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting googletrans (from -r requirements.txt (line 10))\n",
            "  Downloading googletrans-4.0.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 1)) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 1)) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 1)) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 1)) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 1)) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 1)) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 1)) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 1)) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 1)) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 1)) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 1)) (0.37.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 3)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 3)) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 5)) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 5)) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 5)) (3.2.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->-r requirements.txt (line 8)) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->-r requirements.txt (line 8)) (2024.11.6)\n",
            "Requirement already satisfied: httpx>=0.27.2 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.27.2->googletrans->-r requirements.txt (line 10)) (0.28.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 1)) (0.45.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans->-r requirements.txt (line 10)) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans->-r requirements.txt (line 10)) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans->-r requirements.txt (line 10)) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans->-r requirements.txt (line 10)) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans->-r requirements.txt (line 10)) (0.14.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.27.2->googletrans->-r requirements.txt (line 10)) (4.2.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->-r requirements.txt (line 1)) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->-r requirements.txt (line 1)) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->-r requirements.txt (line 1)) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->-r requirements.txt (line 1)) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->-r requirements.txt (line 1)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->-r requirements.txt (line 1)) (3.1.3)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans->-r requirements.txt (line 10)) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans->-r requirements.txt (line 10)) (4.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow->-r requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans->-r requirements.txt (line 10)) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->-r requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->-r requirements.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow->-r requirements.txt (line 1)) (0.1.2)\n",
            "Downloading googletrans-4.0.2-py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=ff8ad1a27e4d97b1e98d795e6bbe56eb58f8e5d80e854c8889db2a6efc88169c\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score, googletrans\n",
            "Successfully installed googletrans-4.0.2 rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7cf8237b-d50a-4dc9-a294-2bb422847ce0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cf8237b-d50a-4dc9-a294-2bb422847ce0",
        "outputId": "8ba78f43-398f-4cf0-b08d-d4949d3b064d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input,\n",
        "    LSTM,\n",
        "    Dense,\n",
        "    Embedding,\n",
        "    Dropout,\n",
        "    LayerNormalization,\n",
        ")\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping,\n",
        "    ReduceLROnPlateau,\n",
        "    ModelCheckpoint\n",
        ")\n",
        "\n",
        "from nltk.translate.bleu_score import (\n",
        "    SmoothingFunction,\n",
        "    corpus_bleu\n",
        ")\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_DIR = \"./Block-Query\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "BEST_MODEL_PATH = os.path.join(SAVE_DIR, \"best_model.keras\")\n",
        "FULL_MODEL_PATH = os.path.join(SAVE_DIR, \"full_model.keras\")\n",
        "ENCODER_MODEL_PATH = os.path.join(SAVE_DIR, \"encoder_model.keras\")\n",
        "DECODER_MODEL_PATH = os.path.join(SAVE_DIR, \"decoder_model.keras\")"
      ],
      "metadata": {
        "id": "V6s40pKOldIU"
      },
      "id": "V6s40pKOldIU",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "190debc0-d689-432d-9bbf-44a497eaaedd",
      "metadata": {
        "id": "190debc0-d689-432d-9bbf-44a497eaaedd",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Final Tuned LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "4788e6e5-433d-4d08-b602-71b5fde3b8ba",
      "metadata": {
        "id": "4788e6e5-433d-4d08-b602-71b5fde3b8ba"
      },
      "outputs": [],
      "source": [
        "MAX_QUESTION_LENGTH = 50\n",
        "MAX_ANSWER_LENGTH = 1500\n",
        "EMBEDDING_DIM = 256\n",
        "LATENT_DIM = 512\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "VALIDATION_SPLIT = 0.2\n",
        "TEST_SPLIT = 0.2\n",
        "LEARNING_RATE = 1e-4\n",
        "CLIP_NORM = 1.0\n",
        "BEAM_WIDTH = 3\n",
        "TEMPERATURE = 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b792c5b-bb43-432c-a168-8c435c85623a",
      "metadata": {
        "id": "6b792c5b-bb43-432c-a168-8c435c85623a"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2e0975c0-ad56-41cd-a7d2-b41c677c4160",
      "metadata": {
        "id": "2e0975c0-ad56-41cd-a7d2-b41c677c4160"
      },
      "outputs": [],
      "source": [
        "def augment_data(question, answer):\n",
        "    augmented_pairs = []\n",
        "    # Original pair\n",
        "    augmented_pairs.append((question, answer))\n",
        "\n",
        "    # Remove punctuation version\n",
        "    q_no_punct = re.sub(r'[.,!?]', '', question)\n",
        "    augmented_pairs.append((q_no_punct, answer))\n",
        "\n",
        "    # Shuffle words slightly (maintaining rough meaning)\n",
        "    words = question.split()\n",
        "    if len(words) > 3:\n",
        "        for i in range(min(3, len(words)-1)):\n",
        "            shuffled = words.copy()\n",
        "            shuffled[i], shuffled[i+1] = shuffled[i+1], shuffled[i]\n",
        "            augmented_pairs.append((' '.join(shuffled), answer))\n",
        "\n",
        "    return augmented_pairs\n",
        "\n",
        "def get_synonyms(word):\n",
        "    try:\n",
        "        synonyms = []\n",
        "        for syn in wordnet.synsets(word):\n",
        "            for lemma in syn.lemmas():\n",
        "                if lemma.name() != word and '_' not in lemma.name():\n",
        "                    synonyms.append(lemma.name())\n",
        "        return list(set(synonyms))\n",
        "    except LookupError:\n",
        "        nltk.download('wordnet')\n",
        "        return get_synonyms(word)\n",
        "    except Exception as e:\n",
        "        print(f\"Synonym error: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def advanced_augment_data(question, answer, augmentation_factor=3):\n",
        "    augmented_pairs = [(question, answer)]\n",
        "\n",
        "    # 1. Synonym replacement\n",
        "    words = question.split()\n",
        "    for _ in range(min(3, len(words))):\n",
        "        new_words = words.copy()\n",
        "        idx = random.randint(0, len(words)-1)\n",
        "        synonyms = get_synonyms(words[idx])\n",
        "        if synonyms:\n",
        "            new_words[idx] = random.choice(synonyms)\n",
        "            augmented_pairs.append((' '.join(new_words), answer))\n",
        "\n",
        "    # 2. Random deletion\n",
        "    if len(words) > 4:\n",
        "        new_words = [word for word in words if random.random() > 0.2]\n",
        "        if new_words:\n",
        "            augmented_pairs.append((' '.join(new_words), answer))\n",
        "\n",
        "    # 3. Word order variations\n",
        "    if len(augmented_pairs) < augmentation_factor:\n",
        "        if len(words) > 3:\n",
        "            for i in range(min(2, len(words)-1)):\n",
        "                shuffled = words.copy()\n",
        "                shuffled[i], shuffled[i+1] = shuffled[i+1], shuffled[i]\n",
        "                augmented_pairs.append((' '.join(shuffled), answer))\n",
        "\n",
        "    # 4. Remove punctuation version\n",
        "    q_no_punct = re.sub(r'[.,!?]', '', question)\n",
        "    if q_no_punct != question:\n",
        "        augmented_pairs.append((q_no_punct, answer))\n",
        "\n",
        "    return augmented_pairs[:augmentation_factor]\n",
        "\n",
        "def load_data(dataset_path):\n",
        "    data = []\n",
        "    for file in sorted(os.listdir(dataset_path)):\n",
        "        if file.endswith(\".json\"):\n",
        "            with open(os.path.join(dataset_path, file), \"r\", encoding=\"utf-8\") as f:\n",
        "                content = json.load(f)\n",
        "                qa_pairs = content.get(\"qa_pairs\", [])\n",
        "                for pair in qa_pairs:\n",
        "                    augmented = augment_data(pair[\"question\"], pair[\"answer\"])\n",
        "                    for q, a in augmented:\n",
        "                        data.append({\"question\": q, \"answer\": a})\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def improved_clean_text(text):\n",
        "    text = text.lower().strip()\n",
        "    # Preserve more meaningful punctuation and symbols\n",
        "    text = re.sub(r'[^\\w\\s.,!?\\'\"-:;$%#@&*()]', ' ', text)\n",
        "    # Normalize numbers\n",
        "    text = re.sub(r'\\d+', 'NUM', text)\n",
        "    # Normalize spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return f\"<START> {text} <END>\"\n",
        "\n",
        "def preprocess_data(df):\n",
        "    df[\"question\"] = df[\"question\"].apply(improved_clean_text)\n",
        "    df[\"answer\"] = df[\"answer\"].apply(improved_clean_text)\n",
        "\n",
        "    # Create tokenizers with additional special tokens\n",
        "    question_tokenizer = Tokenizer(oov_token=\"<UNK>\", filters='')\n",
        "    answer_tokenizer = Tokenizer(oov_token=\"<UNK>\", filters='')\n",
        "\n",
        "    # Add padding token\n",
        "    question_tokenizer.word_index['<PAD>'] = 0\n",
        "    answer_tokenizer.word_index['<PAD>'] = 0\n",
        "\n",
        "    # Fit tokenizers\n",
        "    question_tokenizer.fit_on_texts(df[\"question\"])\n",
        "    answer_tokenizer.fit_on_texts(df[\"answer\"])\n",
        "\n",
        "    # Convert to sequences\n",
        "    question_sequences = question_tokenizer.texts_to_sequences(df[\"question\"])\n",
        "    answer_sequences = answer_tokenizer.texts_to_sequences(df[\"answer\"])\n",
        "\n",
        "    # Pad sequences\n",
        "    question_padded = pad_sequences(question_sequences, maxlen=MAX_QUESTION_LENGTH, padding='post')\n",
        "    answer_padded = pad_sequences(answer_sequences, maxlen=MAX_ANSWER_LENGTH, padding='post')\n",
        "\n",
        "    return question_padded, answer_padded, question_tokenizer, answer_tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0973cf5d-6355-41b9-8c4e-50d8457ec5ed",
      "metadata": {
        "id": "0973cf5d-6355-41b9-8c4e-50d8457ec5ed"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1b0ec4b8-0012-4718-8e09-fb2123f7adc6",
      "metadata": {
        "id": "1b0ec4b8-0012-4718-8e09-fb2123f7adc6"
      },
      "outputs": [],
      "source": [
        "def build_improved_model(vocab_size_q, vocab_size_a):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(MAX_QUESTION_LENGTH,), name='encoder_input')\n",
        "\n",
        "    # Embedding\n",
        "    encoder_embedding = Embedding(\n",
        "        vocab_size_q,\n",
        "        EMBEDDING_DIM,\n",
        "        mask_zero=True,\n",
        "        embeddings_initializer='glorot_uniform',\n",
        "        name='encoder_embedding'\n",
        "    )\n",
        "    encoder_embed = encoder_embedding(encoder_inputs)\n",
        "\n",
        "    # Dropout rate\n",
        "    encoder_dropout1 = Dropout(0.5)(encoder_embed)\n",
        "\n",
        "    encoder_lstm = LSTM(\n",
        "        LATENT_DIM,\n",
        "        return_state=True,\n",
        "        kernel_initializer='glorot_uniform',\n",
        "        recurrent_initializer='orthogonal',\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
        "        recurrent_regularizer=tf.keras.regularizers.l2(1e-4),\n",
        "        name='encoder_lstm'\n",
        "    )\n",
        "\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_dropout1)\n",
        "\n",
        "    # Layer normalization\n",
        "    state_h = LayerNormalization()(state_h)\n",
        "    state_c = LayerNormalization()(state_c)\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(MAX_ANSWER_LENGTH-1,), name='decoder_input')\n",
        "\n",
        "    decoder_embedding = Embedding(\n",
        "        vocab_size_a,\n",
        "        EMBEDDING_DIM,\n",
        "        mask_zero=True,\n",
        "        embeddings_initializer='glorot_uniform',\n",
        "        name='decoder_embedding'\n",
        "    )\n",
        "    decoder_embed = decoder_embedding(decoder_inputs)\n",
        "\n",
        "    decoder_dropout1 = Dropout(0.5)(decoder_embed)\n",
        "\n",
        "    decoder_lstm = LSTM(\n",
        "        LATENT_DIM,\n",
        "        return_sequences=True,\n",
        "        return_state=True,\n",
        "        kernel_initializer='glorot_uniform',\n",
        "        recurrent_initializer='orthogonal',\n",
        "        name='decoder_lstm'\n",
        "    )\n",
        "\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_dropout1, initial_state=[state_h, state_c])\n",
        "\n",
        "    # Layer normalization\n",
        "    decoder_outputs = LayerNormalization()(decoder_outputs)\n",
        "\n",
        "    # Final dropout before dense layer\n",
        "    decoder_dropout2 = Dropout(0.5)(decoder_outputs)\n",
        "\n",
        "    # Dense layer\n",
        "    decoder_dense = Dense(\n",
        "        vocab_size_a,\n",
        "        activation='softmax',\n",
        "        kernel_initializer='glorot_uniform',\n",
        "        name='decoder_dense'\n",
        "    )\n",
        "    decoder_outputs = decoder_dense(decoder_dropout2)\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    # Adam optimizer with gradient clipping\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        clipnorm=CLIP_NORM,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999,\n",
        "        epsilon=1e-7\n",
        "    )\n",
        "\n",
        "    def sparse_categorical_crossentropy_with_smoothing(y_true, y_pred, smoothing=0.1):\n",
        "        num_classes = tf.cast(tf.shape(y_pred)[-1], tf.float32)\n",
        "        y_true = tf.cast(y_true, tf.int32)\n",
        "        y_true_one_hot = tf.one_hot(y_true, tf.shape(y_pred)[-1])\n",
        "\n",
        "        # Apply label smoothing\n",
        "        y_true_smooth = (1.0 - smoothing) * y_true_one_hot + smoothing / num_classes\n",
        "\n",
        "        return tf.reduce_mean(\n",
        "            tf.reduce_sum(-y_true_smooth * tf.math.log(y_pred + 1e-7), axis=-1)\n",
        "        )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=sparse_categorical_crossentropy_with_smoothing,\n",
        "        metrics=['accuracy', 'sparse_categorical_accuracy',\n",
        "         tf.keras.metrics.SparseCategoricalCrossentropy(name='raw_crossentropy')]\n",
        "    )\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "93adade3-ddda-4ec0-bd7f-680bf2632f2f",
      "metadata": {
        "id": "93adade3-ddda-4ec0-bd7f-680bf2632f2f"
      },
      "outputs": [],
      "source": [
        "def create_inference_models(model, vocab_size_a):\n",
        "    encoder_lstm = None\n",
        "    decoder_lstm = None\n",
        "    decoder_dense = None\n",
        "    encoder_embedding = None\n",
        "    decoder_embedding = None\n",
        "\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, LSTM):\n",
        "            if encoder_lstm is None:\n",
        "                encoder_lstm = layer\n",
        "            else:\n",
        "                decoder_lstm = layer\n",
        "        elif isinstance(layer, Dense):\n",
        "            decoder_dense = layer\n",
        "        elif isinstance(layer, Embedding):\n",
        "            if encoder_embedding is None:\n",
        "                encoder_embedding = layer\n",
        "            else:\n",
        "                decoder_embedding = layer\n",
        "\n",
        "    # Create encoder model\n",
        "    encoder_inputs = Input(shape=(MAX_QUESTION_LENGTH,))\n",
        "    x = encoder_embedding(encoder_inputs)\n",
        "    x = Dropout(0.3)(x)\n",
        "    _, state_h, state_c = encoder_lstm(x)\n",
        "    state_h = LayerNormalization()(state_h)\n",
        "    state_c = LayerNormalization()(state_c)\n",
        "    encoder_model = Model(encoder_inputs, [state_h, state_c])\n",
        "\n",
        "    # Create decoder model\n",
        "    decoder_inputs = Input(shape=(1,))\n",
        "    decoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
        "    decoder_state_input_c = Input(shape=(LATENT_DIM,))\n",
        "\n",
        "    x = decoder_embedding(decoder_inputs)\n",
        "    x = Dropout(0.3)(x)\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        x,\n",
        "        initial_state=[decoder_state_input_h, decoder_state_input_c]\n",
        "    )\n",
        "    decoder_outputs = LayerNormalization()(decoder_outputs)\n",
        "    decoder_outputs = Dropout(0.3)(decoder_outputs)\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    decoder_model = Model(\n",
        "        [decoder_inputs, decoder_state_input_h, decoder_state_input_c],\n",
        "        [decoder_outputs, state_h, state_c]\n",
        "    )\n",
        "\n",
        "    return encoder_model, decoder_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation functions"
      ],
      "metadata": {
        "id": "b5k87E6DWbUC"
      },
      "id": "b5k87E6DWbUC"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "fdfc2158-b836-4d5a-ab15-cb905746eaa9",
      "metadata": {
        "id": "fdfc2158-b836-4d5a-ab15-cb905746eaa9"
      },
      "outputs": [],
      "source": [
        "@tf.function(reduce_retracing=True)\n",
        "def predict_step(decoder_model, target_seq, h_batch, c_batch):\n",
        "    return decoder_model([target_seq, h_batch, c_batch], training=False)\n",
        "\n",
        "def optimized_beam_search_decode(encoder_model, decoder_model, input_seq, a_tokenizer,\n",
        "                               beam_width=3, max_length=50, temperature=1.0,\n",
        "                               early_stopping_threshold=0.001):\n",
        "    # Convert input types to tensors with fixed shapes\n",
        "    input_seq = tf.convert_to_tensor(input_seq, dtype=tf.float32)\n",
        "\n",
        "    # Get encoder output (only once)\n",
        "    states_value = encoder_model(input_seq, training=False)\n",
        "\n",
        "    # Initialize beam with start token\n",
        "    start_token = a_tokenizer.word_index['<start>']\n",
        "    end_token = a_tokenizer.word_index['<end>']\n",
        "\n",
        "    beams = [(([start_token], 0.0), states_value)]\n",
        "    finished_beams = []\n",
        "\n",
        "    # Cache for storing decoder outputs\n",
        "    prediction_cache = {}\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        if not beams:\n",
        "            break\n",
        "\n",
        "        # Prepare batched inputs with fixed shapes\n",
        "        current_tokens = np.array([beam[0][0][-1] for beam in beams])\n",
        "        current_states = [beam[1] for beam in beams]\n",
        "\n",
        "        # Create batched inputs\n",
        "        batch_size = len(beams)\n",
        "        target_seq = tf.convert_to_tensor(\n",
        "            current_tokens.reshape(batch_size, 1),\n",
        "            dtype=tf.float32\n",
        "        )\n",
        "\n",
        "        cache_key = tuple(current_tokens)\n",
        "        if cache_key not in prediction_cache:\n",
        "            # Batch predict with fixed shapes\n",
        "            h_batch = tf.convert_to_tensor(\n",
        "                np.vstack([states[0] for states in current_states]),\n",
        "                dtype=tf.float32\n",
        "            )\n",
        "            c_batch = tf.convert_to_tensor(\n",
        "                np.vstack([states[1] for states in current_states]),\n",
        "                dtype=tf.float32\n",
        "            )\n",
        "\n",
        "            outputs_batch = predict_step(\n",
        "                decoder_model,\n",
        "                target_seq,\n",
        "                h_batch,\n",
        "                c_batch\n",
        "            )\n",
        "            prediction_cache[cache_key] = outputs_batch\n",
        "        else:\n",
        "            outputs_batch = prediction_cache[cache_key]\n",
        "\n",
        "        output_tokens_batch, h_batch, c_batch = outputs_batch\n",
        "\n",
        "        candidates = []\n",
        "        for i, ((seq, score), _) in enumerate(beams):\n",
        "            if seq[-1] == end_token:\n",
        "                finished_beams.append((seq, score))\n",
        "                continue\n",
        "\n",
        "            # Apply temperature scaling\n",
        "            logits = tf.cast(output_tokens_batch[i, -1, :], tf.float32)\n",
        "            scaled_logits = logits / temperature\n",
        "            scaled_probs = tf.nn.softmax(scaled_logits).numpy()\n",
        "\n",
        "            # Get top k candidates efficiently\n",
        "            top_k_indices = np.argpartition(scaled_probs, -beam_width)[-beam_width:]\n",
        "            top_k_probs = scaled_probs[top_k_indices]\n",
        "\n",
        "            for idx, prob in zip(top_k_indices, top_k_probs):\n",
        "                new_seq = seq + [idx]\n",
        "                new_score = score + float(tf.math.log(prob + 1e-10))\n",
        "                new_states = [h_batch[i:i+1], c_batch[i:i+1]]\n",
        "                candidates.append(((new_seq, new_score), new_states))\n",
        "\n",
        "        # Early stopping check\n",
        "        if finished_beams:\n",
        "            best_finished_score = max(score for _, score in finished_beams)\n",
        "            candidates = [c for c in candidates\n",
        "                        if c[0][1] + early_stopping_threshold >= best_finished_score]\n",
        "\n",
        "        # Select top beams\n",
        "        candidates.sort(key=lambda x: x[0][1], reverse=True)\n",
        "        beams = candidates[:beam_width]\n",
        "\n",
        "        # Break if all beams finished\n",
        "        if all(beam[0][0][-1] == end_token for beam in beams):\n",
        "            break\n",
        "\n",
        "    # Combine finished and unfinished beams\n",
        "    all_beams = finished_beams + [beam[0] for beam in beams]\n",
        "    best_seq = max(all_beams, key=lambda x: x[1])[0]\n",
        "\n",
        "    return best_seq  # Return the highest scoring sequence\n",
        "\n",
        "# Training history plotting\n",
        "def plot_training_history(history):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Loss plot\n",
        "    axes[0, 0].plot(history.history['loss'], label='Training Loss')\n",
        "    axes[0, 0].plot(history.history['val_loss'], label='Validation Loss')\n",
        "    axes[0, 0].set_title('Model Loss')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "\n",
        "    # Accuracy plot\n",
        "    axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    axes[0, 1].set_title('Model Accuracy')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Accuracy')\n",
        "    axes[0, 1].legend()\n",
        "\n",
        "    # Learning rate plot\n",
        "    axes[1, 0].plot(history.history['learning_rate'], label='Learning Rate')\n",
        "    axes[1, 0].set_title('Learning Rate')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Learning Rate')\n",
        "    axes[1, 0].set_yscale('log')\n",
        "\n",
        "    # Clean up and display\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1cf8d4d6-f5dd-4118-a53a-7c3cd3b53514",
      "metadata": {
        "id": "1cf8d4d6-f5dd-4118-a53a-7c3cd3b53514"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_advanced(encoder_model, decoder_model, q_test, a_test, q_tokenizer, a_tokenizer,\n",
        "                          beam_width=3, temperature=1.0):\n",
        "    smooth = SmoothingFunction().method1\n",
        "    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n",
        "\n",
        "    # Initialize metrics\n",
        "    bleu_scores = {f'bleu_{i}': 0.0 for i in range(1, 5)}\n",
        "    rouge_scores = {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
        "\n",
        "    # Convert test data to tensors once\n",
        "    q_test = tf.convert_to_tensor(q_test, dtype=tf.float32)\n",
        "\n",
        "    # Process in batches for efficiency\n",
        "    batch_size = 32\n",
        "    for i in range(0, len(q_test), batch_size):\n",
        "        batch_end = min(i + batch_size, len(q_test))\n",
        "        batch_q = q_test[i:batch_end]\n",
        "\n",
        "        references = []\n",
        "        hypotheses = []\n",
        "\n",
        "        for j in range(batch_end - i):\n",
        "            # Get reference tokens\n",
        "            reference_tokens = [a_tokenizer.index_word.get(idx, '')\n",
        "                              for idx in a_test[i + j] if idx != 0]\n",
        "            reference_tokens = [token for token in reference_tokens\n",
        "                              if token not in ['<start>', '<end>', '<pad>']]\n",
        "            references.append([reference_tokens])\n",
        "\n",
        "            # Generate response using optimized beam search\n",
        "            decoded_sequence = optimized_beam_search_decode(\n",
        "                encoder_model,\n",
        "                decoder_model,\n",
        "                batch_q[j:j+1],\n",
        "                a_tokenizer,\n",
        "                beam_width=beam_width,\n",
        "                temperature=temperature\n",
        "            )\n",
        "\n",
        "            decoded_tokens = [a_tokenizer.index_word.get(idx, '')\n",
        "                            for idx in decoded_sequence\n",
        "                            if idx not in [a_tokenizer.word_index.get(t, 0)\n",
        "                                         for t in ['<start>', '<end>', '<pad>']]]\n",
        "            hypotheses.append(decoded_tokens)\n",
        "\n",
        "            # Calculate ROUGE scores\n",
        "            rouge_scores_i = rouge_scorer_instance.score(\n",
        "                ' '.join(reference_tokens),\n",
        "                ' '.join(decoded_tokens)\n",
        "            )\n",
        "            for key in rouge_scores:\n",
        "                rouge_scores[key] += rouge_scores_i[key].fmeasure\n",
        "\n",
        "        # Update BLEU scores for batch\n",
        "        for i in range(1, 5):\n",
        "            bleu_scores[f'bleu_{i}'] += corpus_bleu(\n",
        "                references,\n",
        "                hypotheses,\n",
        "                weights=[1.0/i]*i,\n",
        "                smoothing_function=smooth\n",
        "            ) * len(references)\n",
        "\n",
        "    # Normalize scores\n",
        "    n = len(q_test)\n",
        "    bleu_scores = {k: v/n for k, v in bleu_scores.items()}\n",
        "    rouge_scores = {k: v/n for k, v in rouge_scores.items()}\n",
        "\n",
        "    return bleu_scores, rouge_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "7dd83b1b-ed9b-4ed3-84e3-29340384f015",
      "metadata": {
        "id": "7dd83b1b-ed9b-4ed3-84e3-29340384f015"
      },
      "outputs": [],
      "source": [
        "def ask_question(question, encoder_model, decoder_model, q_tokenizer, a_tokenizer,\n",
        "                beam_width=3, temperature=1.0, max_length=1500):\n",
        "    # Preprocess the question\n",
        "    question = improved_clean_text(question)\n",
        "\n",
        "    # Convert to sequence and pad\n",
        "    q_seq = q_tokenizer.texts_to_sequences([question])\n",
        "    q_seq = pad_sequences(q_seq, maxlen=MAX_QUESTION_LENGTH, padding='post')\n",
        "\n",
        "    # Use the optimized beam search for decoding\n",
        "    decoded_sequence = optimized_beam_search_decode(\n",
        "        encoder_model,\n",
        "        decoder_model,\n",
        "        q_seq,\n",
        "        a_tokenizer,\n",
        "        beam_width=beam_width,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    # Convert token indices to words, skipping special tokens\n",
        "    start_token = a_tokenizer.word_index.get('<start>', 0)\n",
        "    end_token = a_tokenizer.word_index.get('<end>', 0)\n",
        "    pad_token = 0\n",
        "\n",
        "    decoded_tokens = []\n",
        "    for idx in decoded_sequence:\n",
        "        # Skip special tokens\n",
        "        if idx not in [start_token, end_token, pad_token]:\n",
        "            word = a_tokenizer.index_word.get(idx, '')\n",
        "            if word not in ['<start>', '<end>', '<pad>', '']:\n",
        "                decoded_tokens.append(word)\n",
        "\n",
        "    return ' '.join(decoded_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "a5a13493-0b9b-46cf-a678-540895785a67",
      "metadata": {
        "id": "a5a13493-0b9b-46cf-a678-540895785a67"
      },
      "outputs": [],
      "source": [
        "class WarmUpLearningRateScheduler(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, warmup_steps, initial_lr):\n",
        "        super().__init__()\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.initial_lr = initial_lr\n",
        "        self.step = 0\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        self.step += 1\n",
        "        if self.step <= self.warmup_steps:\n",
        "            lr = (self.step / self.warmup_steps) * self.initial_lr\n",
        "            self.model.optimizer.learning_rate.assign(lr)\n",
        "\n",
        "callbacks = [\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True,\n",
        "            min_delta=1e-4\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.2,\n",
        "            patience=10,\n",
        "            min_lr=1e-6,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            BEST_MODEL_PATH,\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        WarmUpLearningRateScheduler(warmup_steps=100, initial_lr=LEARNING_RATE),\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and evaluation pipeline"
      ],
      "metadata": {
        "id": "ZQWordwYWnTg"
      },
      "id": "ZQWordwYWnTg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad19a64f-74b9-439f-bc22-5cd2367d479d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ad19a64f-74b9-439f-bc22-5cd2367d479d",
        "outputId": "ccf10127-96b5-47c8-f802-441b4a8ba71d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmented dataset found! Loading directly...\n",
            "Dataset after preprocessing and augmentation:\n",
            "\n",
            "                                        question  \\\n",
            "0   <START> what is blockchain technology? <END>   \n",
            "1   <START> is what blockchain technology? <END>   \n",
            "2   <START> what blockchain is technology? <END>   \n",
            "3    <START> what is blockchain technology <END>   \n",
            "4  <START> what live blockchain technology <END>   \n",
            "\n",
            "                                              answer  \n",
            "0  <START> blockchain is a distributed, decentral...  \n",
            "1  <START> blockchain is a distributed, decentral...  \n",
            "2  <START> blockchain is a distributed, decentral...  \n",
            "3  <START> blockchain is a distributed, decentral...  \n",
            "4  <START> blockchain is a distributed, decentral...  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 38068 entries, 0 to 38067\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   question  38068 non-null  object\n",
            " 1   answer    38068 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 594.9+ KB\n",
            "None\n",
            "Maximum question length (words): 42\n",
            "Maximum answer length (words): 1559\n",
            "Building model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_input             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │      \u001b[38;5;34m1,533,952\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_input             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1499\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ encoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal_2 (\u001b[38;5;33mNotEqual\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1499\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │      \u001b[38;5;34m6,171,136\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │      \u001b[38;5;34m1,574,912\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)]     │                │ not_equal_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1499\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ decoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │          \u001b[38;5;34m1,024\u001b[0m │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_4     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │          \u001b[38;5;34m1,024\u001b[0m │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1499\u001b[0m, \u001b[38;5;34m512\u001b[0m),    │      \u001b[38;5;34m1,574,912\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │                │ layer_normalization_3… │\n",
              "│                           │ \u001b[38;5;34m512\u001b[0m)]                  │                │ layer_normalization_4… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_5     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1499\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │          \u001b[38;5;34m1,024\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1499\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ layer_normalization_5… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_dense (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1499\u001b[0m, \u001b[38;5;34m24106\u001b[0m)    │     \u001b[38;5;34m12,366,378\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_input             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,533,952</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_input             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1499</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1499</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,171,136</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]     │                │ not_equal_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1499</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_4     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1499</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │                │ layer_normalization_3… │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]                  │                │ layer_normalization_4… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_5     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1499</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1499</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_5… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1499</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24106</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,366,378</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,224,362\u001b[0m (88.59 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,224,362</span> (88.59 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m23,224,362\u001b[0m (88.59 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,224,362</span> (88.59 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Training model...\n",
            "Epoch 1/100\n"
          ]
        }
      ],
      "source": [
        "DATASET_FOLDER = \"./Block-Query/dataset\"\n",
        "AUGMENTED_FILE_PATH = os.path.join(DATASET_FOLDER, \"df_augmented.csv\")\n",
        "\n",
        "if os.path.exists(AUGMENTED_FILE_PATH):\n",
        "    print(\"Augmented dataset found! Loading directly...\")\n",
        "    df_augmented = pd.read_csv(AUGMENTED_FILE_PATH)\n",
        "else:\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "    # Load data\n",
        "    df = load_data(\"./Block-Query/dataset\")\n",
        "    print(\"Original Dataset:\\n\")\n",
        "    print(df.head())\n",
        "    print(df.info())\n",
        "\n",
        "    # Preprocess with augmentation\n",
        "    augmented_data = []\n",
        "    for _, row in df.iterrows():\n",
        "        pairs = advanced_augment_data(row['question'], row['answer'])\n",
        "        augmented_data.extend(pairs)\n",
        "\n",
        "    df_augmented = pd.DataFrame(augmented_data, columns=['question', 'answer'])\n",
        "\n",
        "    df_augmented.to_csv(AUGMENTED_FILE_PATH, index=False)\n",
        "    print(f\"Augmented dataset saved at: {AUGMENTED_FILE_PATH}\")\n",
        "\n",
        "# Continue with processing\n",
        "q_data, a_data, q_tokenizer, a_tokenizer = preprocess_data(df_augmented)\n",
        "print(\"Dataset after preprocessing and augmentation:\\n\")\n",
        "print(df_augmented.head())\n",
        "print(df_augmented.info())\n",
        "\n",
        "# Get max length of questions & answers\n",
        "max_question_length = df_augmented[\"question\"].apply(lambda x: len(str(x).split())).max()\n",
        "max_answer_length = df_augmented[\"answer\"].apply(lambda x: len(str(x).split())).max()\n",
        "\n",
        "print(f\"Maximum question length (words): {max_question_length}\")\n",
        "print(f\"Maximum answer length (words): {max_answer_length}\")\n",
        "\n",
        "# Split data\n",
        "indices = np.arange(len(q_data))\n",
        "np.random.shuffle(indices)\n",
        "q_data = q_data[indices]\n",
        "a_data = a_data[indices]\n",
        "\n",
        "num_val = int(len(q_data) * VALIDATION_SPLIT)\n",
        "num_test = int(len(q_data) * TEST_SPLIT)\n",
        "\n",
        "q_train = q_data[:-num_val-num_test]\n",
        "a_train = a_data[:-num_val-num_test]\n",
        "q_val = q_data[-num_val-num_test:-num_test]\n",
        "a_val = a_data[-num_val-num_test:-num_test]\n",
        "q_test = q_data[-num_test:]\n",
        "a_test = a_data[-num_test:]\n",
        "\n",
        "print(\"Building model...\")\n",
        "# Build model\n",
        "model = build_improved_model(len(q_tokenizer.word_index) + 1, len(a_tokenizer.word_index) + 1)\n",
        "\n",
        "print(\"Training model...\")\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    [q_train, a_train[:, :-1]],\n",
        "    a_train[:, 1:],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=([q_val, a_val[:, :-1]], a_val[:, 1:]),\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "print(\"Plotting training history...\")\n",
        "plot_training_history(history)\n",
        "\n",
        "# Create inference models\n",
        "print(\"Creating inference models...\")\n",
        "encoder_model, decoder_model = create_inference_models(model, len(a_tokenizer.word_index) + 1)\n",
        "\n",
        "# Save models\n",
        "print(\"Saving models...\")\n",
        "model.save(FULL_MODEL_PATH)\n",
        "encoder_model.save(ENCODER_MODEL_PATH)\n",
        "decoder_model.save(DECODER_MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49fa035e-740e-4d2b-90cb-9328aecabe33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49fa035e-740e-4d2b-90cb-9328aecabe33",
        "outputId": "aaf6a7c7-ce76-4c63-cf67-9d5dc5de6d33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model...\n",
            "\n",
            "Evaluation Results:\n",
            "BLEU_1: 0.4214\n",
            "BLEU_2: 0.4010\n",
            "BLEU_3: 0.3883\n",
            "BLEU_4: 0.3791\n",
            "ROUGE1: 0.6587\n",
            "ROUGE2: 0.6040\n",
            "ROUGEL: 0.6434\n"
          ]
        }
      ],
      "source": [
        "# Load the encoder and decoder separately\n",
        "encoder_model = load_model(ENCODER_MODEL_PATH)\n",
        "decoder_model = load_model(DECODER_MODEL_PATH)\n",
        "\n",
        "# Evaluate model\n",
        "print(\"Evaluating model...\")\n",
        "bleu_scores, rouge_scores = evaluate_model_advanced(\n",
        "    encoder_model, decoder_model,\n",
        "    q_test, a_test,\n",
        "    q_tokenizer, a_tokenizer,\n",
        "    beam_width=BEAM_WIDTH,\n",
        "    temperature=TEMPERATURE\n",
        ")\n",
        "\n",
        "# Print results\n",
        "print(\"\\nEvaluation Results:\")\n",
        "for metric, score in bleu_scores.items():\n",
        "    print(f\"{metric.upper()}: {score:.4f}\")\n",
        "for metric, score in rouge_scores.items():\n",
        "    print(f\"{metric.upper()}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fafa927-ff7a-40a9-b354-e0be76276aad",
      "metadata": {
        "id": "0fafa927-ff7a-40a9-b354-e0be76276aad",
        "outputId": "7fb33d26-3955-4122-f676-2c81b11cdefc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: blockchain is a decentralized digital currency that operates without a central authority or banks. it enables peer-to-peer transactions on a global scale through a network of computers running the bitcoin protocol. transactions are verified by network nodes through cryptography and recorded in a public distributed ledger called a blockchain. bitcoin\n"
          ]
        }
      ],
      "source": [
        "print(\"Answer:\", ask_question(\"what is blockchain ?\", encoder_model, decoder_model, q_tokenizer, a_tokenizer))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (block-query)",
      "language": "python",
      "name": "block-query"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}